<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>NeRF — Neural Radiance Fields — Research — Pradeep Bolleddu</title>

  <!-- Tailwind for quick layout -->
  <script src="https://cdn.tailwindcss.com"></script>

  <style>
    /* Small extra styles for the SVG labels and page */
    .term-title { font-weight: 700; color: #0f172a; }
    .kbd { font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, "Roboto Mono", monospace; background:#f8fafc; padding:0.1rem 0.35rem; border-radius:4px; }
    /* Make code blocks scrollable on small screens */
    pre.code { background:#0f172a; color:#e6eef6; padding:1rem; border-radius:8px; overflow:auto; }
    .diagram-label { font-family: "Roboto Mono", monospace; font-size:13px; fill:#0f172a; }
    .svg-sample { fill: #0ea5e9; stroke: #0369a1; stroke-width:0.8px; }
    .svg-ray { stroke: #94a3b8; stroke-width:1.2px; stroke-dasharray: 3 2;}
  </style>
</head>
<body class="bg-slate-50 text-slate-800 antialiased">
  <header class="bg-white border-b border-slate-200">
    <div class="max-w-5xl mx-auto px-6 py-5 flex items-center justify-between gap-4">
      <div>
        <a href="index.html" class="text-slate-700 hover:text-slate-900 font-semibold">&larr; Back to Research Index</a>
      </div>
      <h1 class="text-lg font-extrabold">Neural Radiance Fields (NeRF)</h1>
      <div class="text-sm text-slate-500">Pradeep Bolleddu — Research Notes</div>
    </div>
  </header>

  <main class="max-w-5xl mx-auto px-6 py-8 space-y-8">
    <!-- Summary / Hero -->
    <section class="bg-white rounded-xl shadow p-6">
      <h2 class="text-xl font-bold mb-2">Overview</h2>
      <p class="text-slate-700">Neural Radiance Fields (NeRF) represent a continuous volumetric scene function using a neural network. Given a 3D point and a view direction, the network predicts emitted color and volume density; rendering is performed by aggregating samples along camera rays via differentiable volume rendering. NeRFs excel at novel-view synthesis from multi-view images.</p>
    </section>

    <!-- Diagram -->
    <section class="bg-white rounded-xl shadow p-6">
      <h2 class="text-xl font-bold mb-4">Architecture diagram (visual)</h2>
      <p class="text-sm text-slate-600 mb-4">Inline illustrative SVG: camera → rays → sampled 3D points → MLP (positional encoding) → (σ, RGB) → volume rendering → pixel color.</p>

      <div class="flex flex-col md:flex-row gap-6 items-start">
        <div class="md:flex-1">
          <!-- SVG diagram: camera, rays, points, MLP, integration -->
          <svg viewBox="0 0 920 380" class="w-full bg-white rounded-md p-2" aria-hidden="false" role="img">
            <!-- Camera -->
            <g transform="translate(40, 50)">
              <polygon points="0,20 60,0 60,60 0,40" fill="#e6f6ff" stroke="#0369a1" stroke-width="1.4"/>
              <text x="70" y="32" class="diagram-label">Camera (c)</text>
            </g>

            <!-- Rays -->
            <g transform="translate(160, 10)">
              <!-- multiple rays -->
              <line x1="0" y1="25" x2="220" y2="20" class="svg-ray"/>
              <line x1="0" y1="40" x2="220" y2="60" class="svg-ray"/>
              <line x1="0" y1="60" x2="230" y2="120" class="svg-ray"/>
              <text x="10" y="0" class="diagram-label">Primary rays r(t) = o + t·d</text>
            </g>

            <!-- Sample points along ray 1 -->
            <g transform="translate(240, 12)">
              <!-- sample dots on ray 1 -->
              <circle cx="20" cy="12" r="4" class="svg-sample" />
              <circle cx="60" cy="14" r="4" class="svg-sample" />
              <circle cx="100" cy="20" r="4" class="svg-sample" />
              <circle cx="140" cy="30" r="4" class="svg-sample" />
              <text x="0" y="60" class="diagram-label">Sampled 3D points x = (x,y,z), t ∈ [t_n,t_f]</text>
            </g>

            <!-- Positional encoding / MLP box -->
            <g transform="translate(430, 40)">
              <rect x="0" y="0" width="220" height="140" rx="10" fill="#f8fafc" stroke="#0ea5e9" stroke-width="1.6"></rect>
              <text x="12" y="22" class="diagram-label">Positional Encoding γ(x), γ(d)</text>
              <line x1="12" y1="30" x2="12" y2="120" stroke="#cbd5e1" stroke-width="1" />
              <text x="12" y="50" class="diagram-label" style="font-size:12px">→ MLP (fully-connected)</text>

              <!-- small stack layers -->
              <g transform="translate(120,40)">
                <rect x="0" y="0" width="64" height="22" rx="4" fill="#e6f6ff" stroke="#0369a1"></rect>
                <rect x="0" y="28" width="64" height="22" rx="4" fill="#e6f6ff" stroke="#0369a1"></rect>
                <rect x="0" y="56" width="64" height="22" rx="4" fill="#e6f6ff" stroke="#0369a1"></rect>
              </g>

              <text x="16" y="106" class="diagram-label">Outputs: σ (density), c (RGB)</text>
            </g>

            <!-- Integration + Pixel color -->
            <g transform="translate(690, 70)">
              <rect x="0" y="0" width="160" height="120" rx="10" fill="#fff7ed" stroke="#f97316" stroke-width="1.6"></rect>
              <text x="12" y="28" class="diagram-label">Volume Rendering</text>
              <text x="12" y="56" class="diagram-label" style="font-size:13px">C(r) = ∫ T(t)·σ(t)·c(t) dt</text>
              <text x="12" y="84" class="diagram-label" style="font-size:13px">T(t) = exp( -∫ σ(s) ds )</text>

              <!-- final pixel -->
              <rect x="110" y="86" width="36" height="24" fill="#0ea5e9"></rect>
              <text x="110" y="126" class="diagram-label">Pixel color</text>
            </g>

            <!-- labels between MLP and integration -->
            <g transform="translate(620, 60)">
              <line x1="-30" y1="50" x2="0" y2="50" stroke="#94a3b8" stroke-width="1.2"/>
              <text x="-200" y="30" class="diagram-label">(σ, RGB) per sample</text>
            </g>

            <!-- small legend -->
            <g transform="translate(24, 260)">
              <rect x="0" y="0" width="10" height="10" fill="#0ea5e9" stroke="#0369a1"></rect>
              <text x="18" y="10" class="diagram-label">sampled points</text>
            </g>

          </svg>
        </div>

        <div class="md:w-80 bg-slate-50 border border-slate-100 rounded p-4">
          <h4 class="text-sm font-semibold mb-2">Quick facts</h4>
          <ul class="text-sm space-y-2 text-slate-700">
            <li><span class="font-semibold">Inputs:</span> Camera poses + multi-view images</li>
            <li><span class="font-semibold">Core network:</span> MLP with positional encoding</li>
            <li><span class="font-semibold">Output per sample:</span> density σ and color c (RGB)</li>
            <li><span class="font-semibold">Renderer:</span> Differentiable volume rendering (composite integration)</li>
            <li><span class="font-semibold">Goal:</span> minimize photometric reprojection loss</li>
          </ul>
        </div>
      </div>
    </section>

    <!-- Definitions / Concepts -->
    <section class="bg-white rounded-xl shadow p-6">
      <h2 class="text-xl font-bold mb-4">Definitions & Core Concepts</h2>

      <dl class="space-y-4">
        <div>
          <dt class="term-title">Radiance field</dt>
          <dd class="text-slate-700">A continuous function F_Θ(x, d) → (c, σ) that maps a 3D location x ∈ ℝ³ and a viewing direction d to an emitted color c (RGB) and volume density σ (opacity-like scalar).</dd>
        </div>

        <div>
          <dt class="term-title">Positional encoding</dt>
          <dd class="text-slate-700">A mapping γ(x) that converts input coordinates into a higher-frequency representation (sin/cos at multiple frequencies) so the MLP can represent high-frequency detail.</dd>
        </div>

        <div>
          <dt class="term-title">Volume rendering</dt>
          <dd class="text-slate-700">Compute final pixel color by integrating colors along a ray using densities to weight contributions. Discrete approximation: composite samples along the ray using transmittance T and weights w.</dd>
        </div>

        <div>
          <dt class="term-title">Transmittance</dt>
          <dd class="text-slate-700">T(t) = exp( -∫_{t_n}^{t} σ(s) ds ). Intuitively: fraction of light that travels from t to camera without being occluded/absorbed.</dd>
        </div>

        <div>
          <dt class="term-title">Hierarchical sampling</dt>
          <dd class="text-slate-700">Two-stage sampling: coarse network samples many points; importance sampling is used to allocate more samples where contribution is high and refine with a fine network.</dd>
        </div>

        <div>
          <dt class="term-title">Photometric loss</dt>
          <dd class="text-slate-700">Typically L2 (or robust) between rendered pixel colors and observed pixel colors from training views; optionally add regularizers.</dd>
        </div>
      </dl>
    </section>

    <!-- Equations -->
    <section class="bg-white rounded-xl shadow p-6">
      <h2 class="text-xl font-bold mb-3">Key Equations (discrete approximations)</h2>

      <div class="text-sm text-slate-700 space-y-3">
        <div>
          <strong>Ray parameterization:</strong> r(t) = o + t d (o: origin/camera, d: unit direction). Sample t_i along ray.
        </div>
        <div>
          <strong>Volume rendered color (discrete):</strong>
          <pre class="code text-sm p-3 bg-slate-900 rounded"><code>C(r) = ∑_{i=1}^{N} T_i (1 - exp(-σ_i Δ_i)) c_i,
where T_i = exp(-∑_{j=1}^{i-1} σ_j Δ_j)</code></pre>
        </div>
        <div>
          <strong>Loss:</strong> L = ∑_{r in rays} ||C(r) - C_gt(r)||² (plus any auxiliary losses).
        </div>
      </div>
    </section>

    <!-- Pseudocode + Implementation notes -->
    <section class="bg-white rounded-xl shadow p-6">
      <h2 class="text-xl font-bold mb-3">Simple PyTorch-like pseudocode</h2>

      <pre class="code"><code># Pseudocode (high level)
for epoch in range(num_epochs):
    for batch in dataloader:
        rays_o, rays_d, target_rgb = batch  # sample random rays from images
        t_vals_coarse = sample_uniform(t_near, t_far, N_coarse)
        pts_coarse = rays_o[...,None,:] + rays_d[...,None,:] * t_vals_coarse[...,None]
        input_enc = positional_encoding(pts_coarse)       # gamma(x)
        raw_coarse = MLP_coarse(input_enc, viewdirs_enc)  # predicts (rgb, sigma)
        rgb_coarse, sigma_coarse = split(raw_coarse)
        weights_coarse = volume_rendering_weights(sigma_coarse, t_vals_coarse)
        rgb_map_coarse = sum(weights_coarse * rgb_coarse)
        # hierarchical importance sampling using weights_coarse -> t_vals_fine
        pts_fine = rays_o[...,None,:] + rays_d[...,None,:] * t_vals_fine[...,None]
        raw_fine = MLP_fine(positional_encoding(pts_fine), viewdirs_enc)
        rgb_map_fine = volume_rendering(raw_fine, t_vals_fine)
        loss = mse(rgb_map_fine, target_rgb) + mse(rgb_map_coarse, target_rgb)*0.1
        optimizer.zero_grad(); loss.backward(); optimizer.step()</code></pre>

      <h3 class="text-lg font-semibold mt-4">Implementation notes</h3>
      <ul class="list-disc pl-5 mt-2 text-slate-700 space-y-2">
        <li>Use batching over rays (not over whole images) for memory efficiency.</li>
        <li>Use positional encoding with several frequencies (e.g., L=10 for coords, 4–6 for view dir).</li>
        <li>Two-network coarse+fine hierarchical sampling improves quality with fewer samples.</li>
        <li>Instant-NGP style hash encodings drastically speed training — consider if you need speed.</li>
      </ul>
    </section>

    <!-- Datasets / Practical -->
    <section class="bg-white rounded-xl shadow p-6">
      <h2 class="text-xl font-bold mb-3">Datasets, evaluation & practical tips</h2>
      <div class="text-slate-700 space-y-3">
        <p><strong>Common datasets:</strong> Synthetic NeRF dataset (Stanford dragons / lego), LLFF (real forward-facing scenes), Blender synthetic scenes, Tanks & Temples for outdoor scenes.</p>

        <p><strong>Evaluation metrics:</strong> PSNR, SSIM, LPIPS on novel views. Visual fidelity (fine details, occlusions) matters in NeRF work.</p>

        <p><strong>Compute tips:</strong> Training canonical NeRF (without acceleration) can take hours on a single high-end GPU; use smaller MLPs or OpenGL-inspired speedups (instant-ngp) for fast iteration.</p>
      </div>
    </section>

    <!-- Variants -->
    <section class="bg-white rounded-xl shadow p-6">
      <h2 class="text-xl font-bold mb-3">Common NeRF variants & extensions</h2>
      <ul class="list-disc pl-5 text-slate-700 space-y-2">
        <li><strong>mip-NeRF:</strong> Accounts for integrated positional encoding and anti-aliasing for conical frustums.</li>
        <li><strong>NeRF++:</strong> Extends NeRF to unbounded / forward-facing scenes.</li>
        <li><strong>Instant-NGP:</strong> Uses a multiresolution hash/grid encoding for very fast training/rendering.</li>
        <li><strong>PixelNeRF / GRF:</strong> Condition NeRF on a single (or few) images for few-shot novel view synthesis.</li>
        <li><strong>NeRF in the Wild:</strong> Handles uncontrolled lighting conditions and transient objects.</li>
      </ul>
    </section>

    <!-- References & Links -->
    <section class="bg-white rounded-xl shadow p-6">
      <h2 class="text-xl font-bold mb-3">References & further reading</h2>
      <div class="text-sm text-slate-700 space-y-2">
        <p>(Add your local copies of PDF or local pages in this same folder if you prefer.)</p>
        <ul class="list-disc pl-5 mt-2">
          <li><a class="text-blue-600 hover:underline" href="https://arxiv.org/abs/2003.08934" target="_blank" rel="noopener noreferrer">NeRF (original) — Mildenhall et al., 2020 (arXiv)</a></li>
          <li><a class="text-blue-600 hover:underline" href="https://github.com/bmild/nerf" target="_blank" rel="noopener noreferrer">Official / popular NeRF implementations (examples on GitHub)</a></li>
          <li><a class="text-blue-600 hover:underline" href="https://github.com/NVlabs/instant-ngp" target="_blank" rel="noopener noreferrer">Instant-NGP (NVidia / Milestone speedups)</a></li>
        </ul>
      </div>

      <p class="text-xs text-slate-500 mt-4">Tip: if you have local PDF copies, place them into this `research/` folder and reference them as <span class="kbd">./nerf-paper.pdf</span> or similar — that way they open from the same domain (site).</p>
    </section>

    <!-- Small CTA / contact -->
    <section class="text-center text-sm text-slate-600">
      <p>Want me to convert this into a printable one-page summary or add interactive examples (e.g., a WebGL ray-marcher or embedded model viewer)? I can add that to this same folder and link it from here.</p>
    </section>
  </main>

  <footer class="bg-white border-t border-slate-200 mt-10">
    <div class="max-w-5xl mx-auto px-6 py-6 text-xs text-slate-500">
      © <span id="year-footer"></span> Pradeep Bolleddu — NeRF notes. <a href="index.html" class="ml-3 text-slate-600 hover:underline">Research index</a>
    </div>
  </footer>

  <script>
    document.getElementById('year-footer').textContent = new Date().getFullYear();
  </script>
</body>
</html>
